keras0.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  #!/usr/bin/env python
Train on 40000 samples, validate on 20000 samples
Epoch 1/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.7537 - acc: 0.6300Epoch 00000: val_loss improved from inf to 0.07280, saving model to keras_ref1/weights/TrainedModel_PyKeras.h5
40000/40000 [==============================] - 13s - loss: 0.7532 - acc: 0.6303 - val_loss: 0.0728 - val_acc: 0.6766
Epoch 2/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.6088 - acc: 0.6765Epoch 00001: val_loss improved from 0.07280 to 0.07096, saving model to keras_ref1/weights/TrainedModel_PyKeras.h5
40000/40000 [==============================] - 12s - loss: 0.6092 - acc: 0.6765 - val_loss: 0.0710 - val_acc: 0.6929
Epoch 3/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5829 - acc: 0.6957Epoch 00002: val_loss improved from 0.07096 to 0.06945, saving model to keras_ref1/weights/TrainedModel_PyKeras.h5
40000/40000 [==============================] - 12s - loss: 0.5829 - acc: 0.6957 - val_loss: 0.0694 - val_acc: 0.6946
Epoch 4/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.7022Epoch 00003: val_loss improved from 0.06945 to 0.06911, saving model to keras_ref1/weights/TrainedModel_PyKeras.h5
40000/40000 [==============================] - 12s - loss: 0.5745 - acc: 0.7024 - val_loss: 0.0691 - val_acc: 0.6967
Epoch 5/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5697 - acc: 0.7041Epoch 00004: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5699 - acc: 0.7039 - val_loss: 0.0694 - val_acc: 0.6980
Epoch 6/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.7083Epoch 00005: val_loss improved from 0.06911 to 0.06897, saving model to keras_ref1/weights/TrainedModel_PyKeras.h5
40000/40000 [==============================] - 12s - loss: 0.5662 - acc: 0.7082 - val_loss: 0.0690 - val_acc: 0.6996
Epoch 7/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5619 - acc: 0.7103Epoch 00006: val_loss improved from 0.06897 to 0.06892, saving model to keras_ref1/weights/TrainedModel_PyKeras.h5
40000/40000 [==============================] - 12s - loss: 0.5622 - acc: 0.7098 - val_loss: 0.0689 - val_acc: 0.6986
Epoch 8/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5589 - acc: 0.7159Epoch 00007: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5590 - acc: 0.7158 - val_loss: 0.0692 - val_acc: 0.6989
Epoch 9/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.7178Epoch 00008: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5556 - acc: 0.7178 - val_loss: 0.0692 - val_acc: 0.6972
Epoch 10/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.7194Epoch 00009: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5520 - acc: 0.7195 - val_loss: 0.0692 - val_acc: 0.6992
Epoch 11/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7173Epoch 00010: val_loss improved from 0.06892 to 0.06845, saving model to keras_ref1/weights/TrainedModel_PyKeras.h5
40000/40000 [==============================] - 12s - loss: 0.5518 - acc: 0.7174 - val_loss: 0.0684 - val_acc: 0.6976
Epoch 12/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.7225Epoch 00011: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5485 - acc: 0.7221 - val_loss: 0.0689 - val_acc: 0.6978
Epoch 13/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.7223Epoch 00012: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5466 - acc: 0.7222 - val_loss: 0.0692 - val_acc: 0.6967
Epoch 14/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.7262Epoch 00013: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5446 - acc: 0.7265 - val_loss: 0.0691 - val_acc: 0.6985
Epoch 15/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.7244Epoch 00014: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5436 - acc: 0.7244 - val_loss: 0.0690 - val_acc: 0.6988
Epoch 16/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7255Epoch 00015: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5426 - acc: 0.7253 - val_loss: 0.0689 - val_acc: 0.6987
Epoch 17/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5402 - acc: 0.7258Epoch 00016: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5400 - acc: 0.7259 - val_loss: 0.0690 - val_acc: 0.6989
Epoch 18/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.7292Epoch 00017: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5375 - acc: 0.7293 - val_loss: 0.0690 - val_acc: 0.7000
Epoch 19/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7291Epoch 00018: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5361 - acc: 0.7290 - val_loss: 0.0689 - val_acc: 0.6979
Epoch 20/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7305Epoch 00019: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5366 - acc: 0.7305 - val_loss: 0.0690 - val_acc: 0.6971
Epoch 21/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7310Epoch 00020: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5359 - acc: 0.7308 - val_loss: 0.0687 - val_acc: 0.6978
Epoch 22/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5319 - acc: 0.7336Epoch 00021: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5317 - acc: 0.7337 - val_loss: 0.0691 - val_acc: 0.6998
Epoch 23/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.7336Epoch 00022: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5315 - acc: 0.7334 - val_loss: 0.0693 - val_acc: 0.6966
Epoch 24/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.7332Epoch 00023: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5306 - acc: 0.7332 - val_loss: 0.0693 - val_acc: 0.6987
Epoch 25/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.7357Epoch 00024: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5292 - acc: 0.7358 - val_loss: 0.0695 - val_acc: 0.7004
Epoch 26/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5287 - acc: 0.7348Epoch 00025: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5290 - acc: 0.7347 - val_loss: 0.0692 - val_acc: 0.6999
Epoch 27/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5288 - acc: 0.7347Epoch 00026: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5288 - acc: 0.7346 - val_loss: 0.0695 - val_acc: 0.6976
Epoch 28/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.7389Epoch 00027: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5257 - acc: 0.7390 - val_loss: 0.0695 - val_acc: 0.6996
Epoch 29/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7396Epoch 00028: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5243 - acc: 0.7398 - val_loss: 0.0695 - val_acc: 0.6979
Epoch 30/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.7367Epoch 00029: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5252 - acc: 0.7368 - val_loss: 0.0695 - val_acc: 0.6978
Epoch 31/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7375Epoch 00030: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5240 - acc: 0.7378 - val_loss: 0.0694 - val_acc: 0.6964
Epoch 32/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.7405Epoch 00031: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5203 - acc: 0.7405 - val_loss: 0.0698 - val_acc: 0.6971
Epoch 33/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.7430Epoch 00032: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5198 - acc: 0.7430 - val_loss: 0.0698 - val_acc: 0.6992
Epoch 34/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5206 - acc: 0.7415Epoch 00033: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5206 - acc: 0.7415 - val_loss: 0.0696 - val_acc: 0.6982
Epoch 35/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.7438Epoch 00034: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5195 - acc: 0.7437 - val_loss: 0.0695 - val_acc: 0.6948
Epoch 36/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.7419Epoch 00035: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5189 - acc: 0.7418 - val_loss: 0.0701 - val_acc: 0.6983
Epoch 37/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.7449Epoch 00036: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5176 - acc: 0.7448 - val_loss: 0.0697 - val_acc: 0.6958
Epoch 38/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.7431Epoch 00037: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5175 - acc: 0.7432 - val_loss: 0.0700 - val_acc: 0.6966
Epoch 39/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.7461Epoch 00038: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5163 - acc: 0.7461 - val_loss: 0.0698 - val_acc: 0.6952
Epoch 40/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5165 - acc: 0.7458Epoch 00039: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5165 - acc: 0.7458 - val_loss: 0.0701 - val_acc: 0.6944
Epoch 41/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.7461Epoch 00040: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5148 - acc: 0.7457 - val_loss: 0.0696 - val_acc: 0.6952
Epoch 42/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.7472Epoch 00041: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5133 - acc: 0.7472 - val_loss: 0.0701 - val_acc: 0.6959
Epoch 43/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5130 - acc: 0.7462Epoch 00042: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5135 - acc: 0.7458 - val_loss: 0.0700 - val_acc: 0.6934
Epoch 44/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.7471Epoch 00043: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5114 - acc: 0.7471 - val_loss: 0.0700 - val_acc: 0.6953
Epoch 45/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.7480Epoch 00044: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5117 - acc: 0.7481 - val_loss: 0.0701 - val_acc: 0.6936
Epoch 46/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.7452Epoch 00045: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5129 - acc: 0.7453 - val_loss: 0.0701 - val_acc: 0.6963
Epoch 47/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.7482Epoch 00046: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5112 - acc: 0.7482 - val_loss: 0.0705 - val_acc: 0.6953
Epoch 48/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7504Epoch 00047: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5078 - acc: 0.7504 - val_loss: 0.0706 - val_acc: 0.6939
Epoch 49/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.7487Epoch 00048: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5086 - acc: 0.7487 - val_loss: 0.0706 - val_acc: 0.6948
Epoch 50/50
39800/40000 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.7491Epoch 00049: val_loss did not improve
40000/40000 [==============================] - 12s - loss: 0.5097 - acc: 0.7488 - val_loss: 0.0705 - val_acc: 0.6948
                         : Elapsed time for training with 40000 events: 626 sec         
                         : Creating xml weight file: keras_ref1/weights/TMVAClassification_PyKeras.weights.xml
                         : Creating standalone class: keras_ref1/weights/TMVAClassification_PyKeras.class.C
Factory                  : Training finished
                         : 
                         : Ranking input variables (method specific)...
                         : No variable ranking supplied by classifier: PyKeras
Factory                  : === Destroy and recreate all methods via weight files for testing ===
                         : 
Factory                  : Test all methods
Factory                  : Test method: PyKeras for Classification performance
                         : 
                         : Load model from file: keras_ref1/weights/TrainedModel_PyKeras.h5
Factory                  : Evaluate all methods
Factory                  : Evaluate classifier: PyKeras
                         : 
TFHandler_PyKeras        :        Variable               Mean               RMS       [        Min               Max ]
                         : ----------------------------------------------------------------------------------------------
                         :           njets:        -0.029649          0.97630   [          -3.1002           5.7307 ]
                         :        nbjets_m:       -0.0048678           1.0041   [          -5.7307           5.7307 ]
                         :        ncjets_m:        0.0035120          0.98061   [          -3.1942           5.7307 ]
                         :         lepDPhi:         0.013957           1.0218   [          -3.4103           5.7307 ]
                         :       missingET:        0.0054820          0.98946   [          -3.1557           5.7307 ]
                         :         bjetmDR:         0.020515           1.0033   [          -4.0835           5.7307 ]
                         :       bjetmDEta:        0.0081429           1.0054   [          -3.3145           5.7307 ]
                         :       bjetmDPhi:         0.013971           1.0082   [          -3.2748           5.7307 ]
                         :     dibjetsMass:      -0.00053521          0.99061   [          -3.0984           5.7307 ]
                         : bjetPt_dibjetsm:        0.0067203          0.99575   [          -3.0570           5.7307 ]
                         :          cjetPt:       -0.0096552          0.99075   [          -3.6708           5.7307 ]
                         :          jet1pt:        0.0015141          0.98054   [          -3.3405           5.7307 ]
                         :          jet2pt:        -0.015109          0.96906   [          -3.1648           5.7307 ]
                         :          jet3pt:        0.0077534          0.97994   [          -3.1070           5.7307 ]
                         :          jet4pt:       -0.0096345          0.98026   [          -3.1327           3.6538 ]
                         :         jet1eta:        0.0041513          0.98640   [          -3.3151           5.7307 ]
                         :         jet2eta:        0.0035121          0.99509   [          -3.1771           3.5564 ]
                         :         jet3eta:       0.00099431          0.98510   [          -5.7307           5.7307 ]
                         :         jet4eta:        -0.011105          0.98141   [          -3.1651           5.7307 ]
                         :           jet1m:        -0.012546          0.97692   [          -5.7307           4.3826 ]
                         :           jet2m:         0.012413          0.98492   [          -3.1552           5.7307 ]
                         :           jet3m:       0.00060934          0.97595   [          -5.7307           5.7307 ]
                         :           jet4m:       -0.0072443          0.98391   [          -3.1368           5.7307 ]
                         :         jet1csv:         0.011942          0.97832   [          -3.1060           5.7307 ]
                         :         jet2csv:       -0.0031521          0.97848   [          -3.4154           3.5163 ]
                         :         jet3csv:         0.020008          0.97343   [          -3.2079           5.7307 ]
                         :         jet4csv:         0.012550          0.97904   [          -3.5363           5.7307 ]
                         :        jet1cvsl:        -0.010918          0.98663   [          -3.1882           5.7307 ]
                         :        jet2cvsl:        0.0091802          0.98697   [          -5.7307           5.7307 ]
                         :        jet3cvsl:         0.024401          0.98728   [          -3.1057           3.6181 ]
                         :        jet4cvsl:         0.020038          0.98918   [          -3.0565           5.7307 ]
                         :        jet1cvsb:       -0.0095778          0.99314   [          -5.7307           3.6312 ]
                         :        jet2cvsb:        -0.030792           1.0031   [          -3.3920           5.7307 ]
                         :        jet3cvsb:        -0.012012           1.0021   [          -3.5304           5.7307 ]
                         :        jet4cvsb:       -0.0060261           1.0033   [          -3.3322           5.7307 ]
                         :       DRlepWeta:        0.0070702          0.99582   [          -5.7307           5.7307 ]
                         :         DRlepWm:        0.0065716          0.99115   [          -3.1496           5.7307 ]
                         :        DRjet0pt:       -0.0067513          0.99288   [          -3.4233           5.7307 ]
                         :       DRjet0eta:       -0.0076669           1.0047   [          -3.1571           5.7307 ]
                         :         DRjet0m:        0.0011199          0.97282   [          -3.2158           5.7307 ]
                         :       DRjet0csv:        0.0054948          0.98054   [          -3.5760           5.7307 ]
                         :      DRjet0cvsl:         0.017531          0.99362   [          -3.3453           3.9589 ]
                         :      DRjet0cvsb:        -0.030964           1.0069   [          -3.3778           5.7307 ]
                         :        DRjet1pt:         0.023759          0.99149   [          -3.0269           5.7307 ]
                         :       DRjet1eta:        0.0023626           1.0009   [          -3.3144           5.7307 ]
                         :         DRjet1m:         0.013257          0.99783   [          -3.4289           5.7307 ]
                         :       DRjet1csv:       -0.0024451          0.97375   [          -5.7307           5.7307 ]
                         :      DRjet1cvsl:      -0.00033958          0.97821   [          -3.1137           5.7307 ]
                         :      DRjet1cvsb:        -0.010157          0.97697   [          -3.2351           5.7307 ]
                         :        DRjet2pt:        0.0069947          0.99798   [          -3.3042           5.7307 ]
                         :       DRjet2eta:        0.0030921          0.99754   [          -3.3535           5.7307 ]
                         :         DRjet2m:        0.0013975          0.98710   [          -3.1378           5.7307 ]
                         :       DRjet2csv:         0.028045          0.99779   [          -3.3083           3.5297 ]
                         :      DRjet2cvsl:        0.0071155          0.96451   [          -3.2086           5.7307 ]
                         :      DRjet2cvsb:        -0.037776          0.97674   [          -3.1024           5.7307 ]
                         :        DRjet3pt:         0.017252          0.99043   [          -5.7307           5.7307 ]
                         :       DRjet3eta:       -0.0080485          0.98742   [          -3.0556           5.7307 ]
                         :         DRjet3m:         0.021489          0.99693   [          -5.7307           5.7307 ]
                         :       DRjet3csv:         0.027684          0.98500   [          -3.1666           3.7809 ]
                         :      DRjet3cvsl:         0.034837           1.0154   [          -3.1494           5.7307 ]
                         :      DRjet3cvsb:        0.0055828           1.0182   [          -3.1944           5.7307 ]
                         :       DRjet12pt:        -0.012040          0.99880   [          -3.2337           5.7307 ]
                         :      DRjet12eta:        0.0034428           1.0128   [          -3.1475           5.7307 ]
                         :        DRjet12m:         0.013391          0.98287   [          -3.2899           5.7307 ]
                         :       DRjet12DR:         0.020982           1.0049   [          -3.2923           3.8893 ]
                         :       DRjet23pt:       -0.0080373          0.98846   [          -3.2216           5.7307 ]
                         :      DRjet23eta:       -0.0048560           1.0007   [          -3.2847           5.7307 ]
                         :        DRjet23m:        0.0041843          0.99442   [          -3.1943           5.7307 ]
                         :       DRjet31pt:       -0.0059541           1.0007   [          -3.1098           5.7307 ]
                         :      DRjet31eta:        0.0050243           1.0090   [          -3.0975           3.5928 ]
                         :        DRjet31m:        -0.012206          0.98747   [          -3.1170           3.8738 ]
                         :        DRlepTpt:         0.016509          0.98644   [          -3.0449           5.7307 ]
                         :       DRlepTeta:       -0.0070436          0.97311   [          -3.1700           3.7871 ]
                         :         DRlepTm:        -0.024244          0.98045   [          -3.1628           5.7307 ]
                         :        DRhadTpt:       -0.0088592          0.98976   [          -5.7307           3.5936 ]
                         :       DRhadTeta:        0.0032784           1.0035   [          -3.2976           5.7307 ]
                         :         DRhadTm:         0.016361          0.97891   [          -3.1102           5.7307 ]
                         : ----------------------------------------------------------------------------------------------
PyKeras                  : [keras_ref1] : Loop over test events and fill histograms with classifier response...
                         : 
TFHandler_PyKeras        :        Variable               Mean               RMS       [        Min               Max ]
                         : ----------------------------------------------------------------------------------------------
                         :           njets:        -0.029649          0.97630   [          -3.1002           5.7307 ]
                         :        nbjets_m:       -0.0048678           1.0041   [          -5.7307           5.7307 ]
                         :        ncjets_m:        0.0035120          0.98061   [          -3.1942           5.7307 ]
                         :         lepDPhi:         0.013957           1.0218   [          -3.4103           5.7307 ]
                         :       missingET:        0.0054820          0.98946   [          -3.1557           5.7307 ]
                         :         bjetmDR:         0.020515           1.0033   [          -4.0835           5.7307 ]
                         :       bjetmDEta:        0.0081429           1.0054   [          -3.3145           5.7307 ]
                         :       bjetmDPhi:         0.013971           1.0082   [          -3.2748           5.7307 ]
                         :     dibjetsMass:      -0.00053521          0.99061   [          -3.0984           5.7307 ]
                         : bjetPt_dibjetsm:        0.0067203          0.99575   [          -3.0570           5.7307 ]
                         :          cjetPt:       -0.0096552          0.99075   [          -3.6708           5.7307 ]
                         :          jet1pt:        0.0015141          0.98054   [          -3.3405           5.7307 ]
                         :          jet2pt:        -0.015109          0.96906   [          -3.1648           5.7307 ]
                         :          jet3pt:        0.0077534          0.97994   [          -3.1070           5.7307 ]
                         :          jet4pt:       -0.0096345          0.98026   [          -3.1327           3.6538 ]
                         :         jet1eta:        0.0041513          0.98640   [          -3.3151           5.7307 ]
                         :         jet2eta:        0.0035121          0.99509   [          -3.1771           3.5564 ]
                         :         jet3eta:       0.00099431          0.98510   [          -5.7307           5.7307 ]
                         :         jet4eta:        -0.011105          0.98141   [          -3.1651           5.7307 ]
                         :           jet1m:        -0.012546          0.97692   [          -5.7307           4.3826 ]
                         :           jet2m:         0.012413          0.98492   [          -3.1552           5.7307 ]
                         :           jet3m:       0.00060934          0.97595   [          -5.7307           5.7307 ]
                         :           jet4m:       -0.0072443          0.98391   [          -3.1368           5.7307 ]
                         :         jet1csv:         0.011942          0.97832   [          -3.1060           5.7307 ]
                         :         jet2csv:       -0.0031521          0.97848   [          -3.4154           3.5163 ]
                         :         jet3csv:         0.020008          0.97343   [          -3.2079           5.7307 ]
                         :         jet4csv:         0.012550          0.97904   [          -3.5363           5.7307 ]
                         :        jet1cvsl:        -0.010918          0.98663   [          -3.1882           5.7307 ]
                         :        jet2cvsl:        0.0091802          0.98697   [          -5.7307           5.7307 ]
                         :        jet3cvsl:         0.024401          0.98728   [          -3.1057           3.6181 ]
                         :        jet4cvsl:         0.020038          0.98918   [          -3.0565           5.7307 ]
                         :        jet1cvsb:       -0.0095778          0.99314   [          -5.7307           3.6312 ]
                         :        jet2cvsb:        -0.030792           1.0031   [          -3.3920           5.7307 ]
                         :        jet3cvsb:        -0.012012           1.0021   [          -3.5304           5.7307 ]
                         :        jet4cvsb:       -0.0060261           1.0033   [          -3.3322           5.7307 ]
                         :       DRlepWeta:        0.0070702          0.99582   [          -5.7307           5.7307 ]
                         :         DRlepWm:        0.0065716          0.99115   [          -3.1496           5.7307 ]
                         :        DRjet0pt:       -0.0067513          0.99288   [          -3.4233           5.7307 ]
                         :       DRjet0eta:       -0.0076669           1.0047   [          -3.1571           5.7307 ]
                         :         DRjet0m:        0.0011199          0.97282   [          -3.2158           5.7307 ]
                         :       DRjet0csv:        0.0054948          0.98054   [          -3.5760           5.7307 ]
                         :      DRjet0cvsl:         0.017531          0.99362   [          -3.3453           3.9589 ]
                         :      DRjet0cvsb:        -0.030964           1.0069   [          -3.3778           5.7307 ]
                         :        DRjet1pt:         0.023759          0.99149   [          -3.0269           5.7307 ]
                         :       DRjet1eta:        0.0023626           1.0009   [          -3.3144           5.7307 ]
                         :         DRjet1m:         0.013257          0.99783   [          -3.4289           5.7307 ]
                         :       DRjet1csv:       -0.0024451          0.97375   [          -5.7307           5.7307 ]
                         :      DRjet1cvsl:      -0.00033958          0.97821   [          -3.1137           5.7307 ]
                         :      DRjet1cvsb:        -0.010157          0.97697   [          -3.2351           5.7307 ]
                         :        DRjet2pt:        0.0069947          0.99798   [          -3.3042           5.7307 ]
                         :       DRjet2eta:        0.0030921          0.99754   [          -3.3535           5.7307 ]
                         :         DRjet2m:        0.0013975          0.98710   [          -3.1378           5.7307 ]
                         :       DRjet2csv:         0.028045          0.99779   [          -3.3083           3.5297 ]
                         :      DRjet2cvsl:        0.0071155          0.96451   [          -3.2086           5.7307 ]
                         :      DRjet2cvsb:        -0.037776          0.97674   [          -3.1024           5.7307 ]
                         :        DRjet3pt:         0.017252          0.99043   [          -5.7307           5.7307 ]
                         :       DRjet3eta:       -0.0080485          0.98742   [          -3.0556           5.7307 ]
                         :         DRjet3m:         0.021489          0.99693   [          -5.7307           5.7307 ]
                         :       DRjet3csv:         0.027684          0.98500   [          -3.1666           3.7809 ]
                         :      DRjet3cvsl:         0.034837           1.0154   [          -3.1494           5.7307 ]
                         :      DRjet3cvsb:        0.0055828           1.0182   [          -3.1944           5.7307 ]
                         :       DRjet12pt:        -0.012040          0.99880   [          -3.2337           5.7307 ]
                         :      DRjet12eta:        0.0034428           1.0128   [          -3.1475           5.7307 ]
                         :        DRjet12m:         0.013391          0.98287   [          -3.2899           5.7307 ]
                         :       DRjet12DR:         0.020982           1.0049   [          -3.2923           3.8893 ]
                         :       DRjet23pt:       -0.0080373          0.98846   [          -3.2216           5.7307 ]
                         :      DRjet23eta:       -0.0048560           1.0007   [          -3.2847           5.7307 ]
                         :        DRjet23m:        0.0041843          0.99442   [          -3.1943           5.7307 ]
                         :       DRjet31pt:       -0.0059541           1.0007   [          -3.1098           5.7307 ]
                         :      DRjet31eta:        0.0050243           1.0090   [          -3.0975           3.5928 ]
                         :        DRjet31m:        -0.012206          0.98747   [          -3.1170           3.8738 ]
                         :        DRlepTpt:         0.016509          0.98644   [          -3.0449           5.7307 ]
                         :       DRlepTeta:       -0.0070436          0.97311   [          -3.1700           3.7871 ]
                         :         DRlepTm:        -0.024244          0.98045   [          -3.1628           5.7307 ]
                         :        DRhadTpt:       -0.0088592          0.98976   [          -5.7307           3.5936 ]
                         :       DRhadTeta:        0.0032784           1.0035   [          -3.2976           5.7307 ]
                         :         DRhadTm:         0.016361          0.97891   [          -3.1102           5.7307 ]
                         : ----------------------------------------------------------------------------------------------
                         : 
                         : <PlotVariables> Will not produce scatter plots ==> 
                         : |  The number of 77 input variables and 0 target values would require 2926 two-dimensional
                         : |  histograms, which would occupy the computer's memory. Note that this
                         : |  suppression does not have any consequences for your analysis, other
                         : |  than not disposing of these scatter plots. You can modify the maximum
                         : |  number of input variables allowed to generate scatter plots in your
                         : |  script via the command line:
                         : |  "(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;"
                         : 
                         : Some more output
                         : 
                         : Evaluation results ranked by best signal efficiency and purity (area)
                         : -------------------------------------------------------------------------------------------------------------------
                         : DataSet       MVA                       
                         : Name:         Method:          ROC-integ
                         : keras_ref1    PyKeras        : 0.774
                         : -------------------------------------------------------------------------------------------------------------------
                         : 
                         : Testing efficiency compared to training efficiency (overtraining check)
                         : -------------------------------------------------------------------------------------------------------------------
                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) 
                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   
                         : -------------------------------------------------------------------------------------------------------------------
                         : keras_ref1           PyKeras        : 0.147 (0.172)       0.445 (0.484)      0.707 (0.730)
                         : -------------------------------------------------------------------------------------------------------------------
                         : 
Dataset:keras_ref1       : Created tree 'TestTree' with 20000 events
                         : 
Dataset:keras_ref1       : Created tree 'TrainTree' with 40000 events
                         : 
Factory                  : Thank you for using TMVA!
                         : For citation information, please visit: http://tmva.sf.net/citeTMVA.html

