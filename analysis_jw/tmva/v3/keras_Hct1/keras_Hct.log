Train on 70000 samples, validate on 20000 samples
Epoch 1/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.7258 - acc: 0.5673Epoch 00000: val_loss improved from inf to 0.07481, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.7257 - acc: 0.5673 - val_loss: 0.0748 - val_acc: 0.6407
Epoch 2/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6516 - acc: 0.6245Epoch 00001: val_loss improved from 0.07481 to 0.07324, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6516 - acc: 0.6246 - val_loss: 0.0732 - val_acc: 0.6462
Epoch 3/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6407 - acc: 0.6391Epoch 00002: val_loss improved from 0.07324 to 0.07274, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6406 - acc: 0.6390 - val_loss: 0.0727 - val_acc: 0.6492
Epoch 4/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6371 - acc: 0.6425Epoch 00003: val_loss improved from 0.07274 to 0.07249, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6371 - acc: 0.6425 - val_loss: 0.0725 - val_acc: 0.6483
Epoch 5/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6333 - acc: 0.6460Epoch 00004: val_loss improved from 0.07249 to 0.07232, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6333 - acc: 0.6460 - val_loss: 0.0723 - val_acc: 0.6478
Epoch 6/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6324 - acc: 0.6471Epoch 00005: val_loss improved from 0.07232 to 0.07175, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6325 - acc: 0.6470 - val_loss: 0.0717 - val_acc: 0.6506
Epoch 7/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6309 - acc: 0.6474Epoch 00006: val_loss improved from 0.07175 to 0.07172, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6308 - acc: 0.6474 - val_loss: 0.0717 - val_acc: 0.6490
Epoch 8/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6299 - acc: 0.6496Epoch 00007: val_loss improved from 0.07172 to 0.07151, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6299 - acc: 0.6495 - val_loss: 0.0715 - val_acc: 0.6501
Epoch 9/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6282 - acc: 0.6512Epoch 00008: val_loss improved from 0.07151 to 0.07141, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6282 - acc: 0.6511 - val_loss: 0.0714 - val_acc: 0.6499
Epoch 10/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6290 - acc: 0.6517Epoch 00009: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6289 - acc: 0.6515 - val_loss: 0.0717 - val_acc: 0.6492
Epoch 11/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6271 - acc: 0.6508Epoch 00010: val_loss improved from 0.07141 to 0.07107, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6271 - acc: 0.6509 - val_loss: 0.0711 - val_acc: 0.6505
Epoch 12/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6264 - acc: 0.6514Epoch 00011: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6265 - acc: 0.6513 - val_loss: 0.0716 - val_acc: 0.6505
Epoch 13/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6253 - acc: 0.6528Epoch 00012: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6253 - acc: 0.6528 - val_loss: 0.0713 - val_acc: 0.6508
Epoch 14/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6239 - acc: 0.6538Epoch 00013: val_loss improved from 0.07107 to 0.07102, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6240 - acc: 0.6539 - val_loss: 0.0710 - val_acc: 0.6532
Epoch 15/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6244 - acc: 0.6540Epoch 00014: val_loss improved from 0.07102 to 0.07073, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6245 - acc: 0.6540 - val_loss: 0.0707 - val_acc: 0.6545
Epoch 16/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6230 - acc: 0.6562Epoch 00015: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6232 - acc: 0.6561 - val_loss: 0.0711 - val_acc: 0.6543
Epoch 17/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6218 - acc: 0.6574Epoch 00016: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6219 - acc: 0.6574 - val_loss: 0.0711 - val_acc: 0.6545
Epoch 18/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6221 - acc: 0.6566Epoch 00017: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6220 - acc: 0.6568 - val_loss: 0.0709 - val_acc: 0.6543
Epoch 19/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6211 - acc: 0.6574Epoch 00018: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6210 - acc: 0.6576 - val_loss: 0.0709 - val_acc: 0.6548
Epoch 20/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6210 - acc: 0.6585Epoch 00019: val_loss improved from 0.07073 to 0.07061, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6211 - acc: 0.6583 - val_loss: 0.0706 - val_acc: 0.6544
Epoch 21/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6195 - acc: 0.6585Epoch 00020: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6195 - acc: 0.6584 - val_loss: 0.0707 - val_acc: 0.6549
Epoch 22/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6185 - acc: 0.6584Epoch 00021: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6185 - acc: 0.6584 - val_loss: 0.0708 - val_acc: 0.6550
Epoch 23/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6165 - acc: 0.6624Epoch 00022: val_loss improved from 0.07061 to 0.07043, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6168 - acc: 0.6621 - val_loss: 0.0704 - val_acc: 0.6569
Epoch 24/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6180 - acc: 0.6618Epoch 00023: val_loss improved from 0.07043 to 0.07035, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6179 - acc: 0.6619 - val_loss: 0.0703 - val_acc: 0.6572
Epoch 25/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6156 - acc: 0.6640Epoch 00024: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6156 - acc: 0.6640 - val_loss: 0.0704 - val_acc: 0.6586
Epoch 26/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6159 - acc: 0.6617Epoch 00025: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6160 - acc: 0.6616 - val_loss: 0.0707 - val_acc: 0.6577
Epoch 27/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6149 - acc: 0.6647Epoch 00026: val_loss improved from 0.07035 to 0.07032, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6148 - acc: 0.6647 - val_loss: 0.0703 - val_acc: 0.6605
Epoch 28/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6149 - acc: 0.6643Epoch 00027: val_loss improved from 0.07032 to 0.07001, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6151 - acc: 0.6641 - val_loss: 0.0700 - val_acc: 0.6610
Epoch 29/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6145 - acc: 0.6643Epoch 00028: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6145 - acc: 0.6643 - val_loss: 0.0703 - val_acc: 0.6603
Epoch 30/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6131 - acc: 0.6657Epoch 00029: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6131 - acc: 0.6657 - val_loss: 0.0701 - val_acc: 0.6622
Epoch 31/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6121 - acc: 0.6670Epoch 00030: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6121 - acc: 0.6670 - val_loss: 0.0701 - val_acc: 0.6638
Epoch 32/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6122 - acc: 0.6656Epoch 00031: val_loss improved from 0.07001 to 0.07000, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6122 - acc: 0.6656 - val_loss: 0.0700 - val_acc: 0.6649
Epoch 33/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6114 - acc: 0.6675Epoch 00032: val_loss improved from 0.07000 to 0.06998, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6113 - acc: 0.6676 - val_loss: 0.0700 - val_acc: 0.6644
Epoch 34/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6107 - acc: 0.6669Epoch 00033: val_loss improved from 0.06998 to 0.06988, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6107 - acc: 0.6670 - val_loss: 0.0699 - val_acc: 0.6661
Epoch 35/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.6697Epoch 00034: val_loss improved from 0.06988 to 0.06971, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6103 - acc: 0.6696 - val_loss: 0.0697 - val_acc: 0.6656
Epoch 36/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6092 - acc: 0.6687Epoch 00035: val_loss improved from 0.06971 to 0.06964, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6091 - acc: 0.6688 - val_loss: 0.0696 - val_acc: 0.6662
Epoch 37/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6088 - acc: 0.6696Epoch 00036: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.6088 - acc: 0.6697 - val_loss: 0.0700 - val_acc: 0.6643
Epoch 38/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6083 - acc: 0.6708Epoch 00037: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.6083 - acc: 0.6707 - val_loss: 0.0698 - val_acc: 0.6656
Epoch 39/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6083 - acc: 0.6705Epoch 00038: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.6083 - acc: 0.6705 - val_loss: 0.0700 - val_acc: 0.6657
Epoch 40/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6079 - acc: 0.6712Epoch 00039: val_loss improved from 0.06964 to 0.06960, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 24s - loss: 0.6079 - acc: 0.6711 - val_loss: 0.0696 - val_acc: 0.6667
Epoch 41/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6071 - acc: 0.6709Epoch 00040: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6074 - acc: 0.6708 - val_loss: 0.0699 - val_acc: 0.6653
Epoch 42/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6068 - acc: 0.6720Epoch 00041: val_loss improved from 0.06960 to 0.06948, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6068 - acc: 0.6721 - val_loss: 0.0695 - val_acc: 0.6668
Epoch 43/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6064 - acc: 0.6722Epoch 00042: val_loss improved from 0.06948 to 0.06941, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6065 - acc: 0.6721 - val_loss: 0.0694 - val_acc: 0.6672
Epoch 44/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6062 - acc: 0.6735Epoch 00043: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6061 - acc: 0.6735 - val_loss: 0.0696 - val_acc: 0.6668
Epoch 45/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6063 - acc: 0.6724Epoch 00044: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6064 - acc: 0.6724 - val_loss: 0.0699 - val_acc: 0.6657
Epoch 46/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6049 - acc: 0.6738Epoch 00045: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6048 - acc: 0.6739 - val_loss: 0.0695 - val_acc: 0.6676
Epoch 47/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6064 - acc: 0.6720Epoch 00046: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6064 - acc: 0.6720 - val_loss: 0.0697 - val_acc: 0.6665
Epoch 48/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.6759Epoch 00047: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6031 - acc: 0.6761 - val_loss: 0.0698 - val_acc: 0.6648
Epoch 49/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6041 - acc: 0.6731Epoch 00048: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6040 - acc: 0.6732 - val_loss: 0.0695 - val_acc: 0.6668
Epoch 50/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6042 - acc: 0.6740Epoch 00049: val_loss improved from 0.06941 to 0.06937, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6041 - acc: 0.6740 - val_loss: 0.0694 - val_acc: 0.6666
Epoch 51/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.6745Epoch 00050: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6031 - acc: 0.6745 - val_loss: 0.0695 - val_acc: 0.6657
Epoch 52/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.6768Epoch 00051: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6023 - acc: 0.6767 - val_loss: 0.0698 - val_acc: 0.6641
Epoch 53/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.6754Epoch 00052: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6022 - acc: 0.6756 - val_loss: 0.0694 - val_acc: 0.6663
Epoch 54/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6005 - acc: 0.6784Epoch 00053: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6005 - acc: 0.6786 - val_loss: 0.0695 - val_acc: 0.6652
Epoch 55/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.6767Epoch 00054: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6016 - acc: 0.6767 - val_loss: 0.0694 - val_acc: 0.6656
Epoch 56/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.6768Epoch 00055: val_loss improved from 0.06937 to 0.06934, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.6022 - acc: 0.6768 - val_loss: 0.0693 - val_acc: 0.6670
Epoch 57/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6008 - acc: 0.6773Epoch 00056: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.6009 - acc: 0.6773 - val_loss: 0.0694 - val_acc: 0.6659
Epoch 58/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6003 - acc: 0.6771Epoch 00057: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.6002 - acc: 0.6770 - val_loss: 0.0695 - val_acc: 0.6662
Epoch 59/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6008 - acc: 0.6772Epoch 00058: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6008 - acc: 0.6773 - val_loss: 0.0695 - val_acc: 0.6670
Epoch 60/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.6770Epoch 00059: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.6012 - acc: 0.6770 - val_loss: 0.0695 - val_acc: 0.6675
Epoch 61/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6002 - acc: 0.6771Epoch 00060: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.6001 - acc: 0.6773 - val_loss: 0.0694 - val_acc: 0.6676
Epoch 62/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.6777Epoch 00061: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.6010 - acc: 0.6778 - val_loss: 0.0694 - val_acc: 0.6674
Epoch 63/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5999 - acc: 0.6788Epoch 00062: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.5998 - acc: 0.6789 - val_loss: 0.0694 - val_acc: 0.6683
Epoch 64/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5999 - acc: 0.6787Epoch 00063: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5998 - acc: 0.6788 - val_loss: 0.0695 - val_acc: 0.6671
Epoch 65/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.6007 - acc: 0.6759Epoch 00064: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.6007 - acc: 0.6759 - val_loss: 0.0696 - val_acc: 0.6663
Epoch 66/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5997 - acc: 0.6781Epoch 00065: val_loss improved from 0.06934 to 0.06926, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.5997 - acc: 0.6781 - val_loss: 0.0693 - val_acc: 0.6679
Epoch 67/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5994 - acc: 0.6813Epoch 00066: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5995 - acc: 0.6813 - val_loss: 0.0696 - val_acc: 0.6673
Epoch 68/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5975 - acc: 0.6794Epoch 00067: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5976 - acc: 0.6794 - val_loss: 0.0694 - val_acc: 0.6671
Epoch 69/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5984 - acc: 0.6787Epoch 00068: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.5984 - acc: 0.6786 - val_loss: 0.0693 - val_acc: 0.6674
Epoch 70/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5985 - acc: 0.6811Epoch 00069: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5984 - acc: 0.6812 - val_loss: 0.0695 - val_acc: 0.6674
Epoch 71/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5972 - acc: 0.6817Epoch 00070: val_loss did not improve
70000/70000 [==============================] - 24s - loss: 0.5972 - acc: 0.6818 - val_loss: 0.0696 - val_acc: 0.6678
Epoch 72/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5979 - acc: 0.6798Epoch 00071: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5979 - acc: 0.6798 - val_loss: 0.0697 - val_acc: 0.6671
Epoch 73/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5986 - acc: 0.6796Epoch 00072: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5985 - acc: 0.6796 - val_loss: 0.0693 - val_acc: 0.6690
Epoch 74/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5982 - acc: 0.6806Epoch 00073: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5982 - acc: 0.6807 - val_loss: 0.0694 - val_acc: 0.6688
Epoch 75/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5981 - acc: 0.6820Epoch 00074: val_loss improved from 0.06926 to 0.06918, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.5980 - acc: 0.6820 - val_loss: 0.0692 - val_acc: 0.6697
Epoch 76/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5981 - acc: 0.6788Epoch 00075: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5980 - acc: 0.6787 - val_loss: 0.0694 - val_acc: 0.6689
Epoch 77/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5968 - acc: 0.6790Epoch 00076: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5970 - acc: 0.6788 - val_loss: 0.0695 - val_acc: 0.6686
Epoch 78/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5970 - acc: 0.6807Epoch 00077: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5971 - acc: 0.6806 - val_loss: 0.0694 - val_acc: 0.6686
Epoch 79/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5966 - acc: 0.6809Epoch 00078: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5965 - acc: 0.6810 - val_loss: 0.0693 - val_acc: 0.6689
Epoch 80/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5965 - acc: 0.6804Epoch 00079: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5964 - acc: 0.6805 - val_loss: 0.0693 - val_acc: 0.6690
Epoch 81/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5960 - acc: 0.6828Epoch 00080: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5962 - acc: 0.6827 - val_loss: 0.0693 - val_acc: 0.6688
Epoch 82/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5967 - acc: 0.6808Epoch 00081: val_loss improved from 0.06918 to 0.06909, saving model to keras_Hct1/weights/TrainedModel_PyKeras.h5
70000/70000 [==============================] - 25s - loss: 0.5966 - acc: 0.6809 - val_loss: 0.0691 - val_acc: 0.6690
Epoch 83/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5954 - acc: 0.6814Epoch 00082: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5954 - acc: 0.6813 - val_loss: 0.0693 - val_acc: 0.6694
Epoch 84/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5965 - acc: 0.6802Epoch 00083: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5965 - acc: 0.6802 - val_loss: 0.0692 - val_acc: 0.6693
Epoch 85/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.6809Epoch 00084: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5959 - acc: 0.6811 - val_loss: 0.0692 - val_acc: 0.6692
Epoch 86/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5965 - acc: 0.6812Epoch 00085: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5963 - acc: 0.6813 - val_loss: 0.0693 - val_acc: 0.6690
Epoch 87/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5952 - acc: 0.6837Epoch 00086: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5952 - acc: 0.6836 - val_loss: 0.0693 - val_acc: 0.6686
Epoch 88/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5957 - acc: 0.6827Epoch 00087: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5958 - acc: 0.6826 - val_loss: 0.0693 - val_acc: 0.6690
Epoch 89/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5942 - acc: 0.6837Epoch 00088: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5943 - acc: 0.6836 - val_loss: 0.0691 - val_acc: 0.6687
Epoch 90/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.6826Epoch 00089: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5958 - acc: 0.6826 - val_loss: 0.0692 - val_acc: 0.6691
Epoch 91/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5945 - acc: 0.6825Epoch 00090: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5945 - acc: 0.6825 - val_loss: 0.0691 - val_acc: 0.6690
Epoch 92/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5956 - acc: 0.6823Epoch 00091: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5954 - acc: 0.6823 - val_loss: 0.0693 - val_acc: 0.6687
Epoch 93/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.6824Epoch 00092: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5941 - acc: 0.6823 - val_loss: 0.0692 - val_acc: 0.6692
Epoch 94/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5955 - acc: 0.6823Epoch 00093: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5954 - acc: 0.6824 - val_loss: 0.0693 - val_acc: 0.6692
Epoch 95/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5953 - acc: 0.6821Epoch 00094: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5953 - acc: 0.6821 - val_loss: 0.0693 - val_acc: 0.6685
Epoch 96/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5942 - acc: 0.6840Epoch 00095: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5942 - acc: 0.6841 - val_loss: 0.0695 - val_acc: 0.6682
Epoch 97/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.6817Epoch 00096: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5941 - acc: 0.6817 - val_loss: 0.0692 - val_acc: 0.6696
Epoch 98/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5939 - acc: 0.6841Epoch 00097: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5939 - acc: 0.6840 - val_loss: 0.0691 - val_acc: 0.6699
Epoch 99/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5948 - acc: 0.6842Epoch 00098: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5948 - acc: 0.6841 - val_loss: 0.0694 - val_acc: 0.6696
Epoch 100/100
69800/70000 [============================>.] - ETA: 0s - loss: 0.5933 - acc: 0.6835Epoch 00099: val_loss did not improve
70000/70000 [==============================] - 25s - loss: 0.5934 - acc: 0.6834 - val_loss: 0.0693 - val_acc: 0.6693
                         : Elapsed time for training with 70000 events: 2.53e+03 sec         
                         : Creating xml weight file: keras_Hct1/weights/TMVAClassification_PyKeras.weights.xml
                         : Creating standalone class: keras_Hct1/weights/TMVAClassification_PyKeras.class.C
Factory                  : Training finished
                         : 
                         : Ranking input variables (method specific)...
BDT                      : Ranking result (top variable is best ranked)
                         : -------------------------------------------------
                         : Rank : Variable        : Variable Importance
                         : -------------------------------------------------
                         :    1 : DRjet2cvsb      : 2.209e-02
                         :    2 : DRjet3cvsl      : 2.174e-02
                         :    3 : DRjet12m        : 1.964e-02
                         :    4 : DRjet0cvsb      : 1.925e-02
                         :    5 : DRjet2csv       : 1.919e-02
                         :    6 : DRjet0cvsl      : 1.884e-02
                         :    7 : DRjet3csv       : 1.864e-02
                         :    8 : DRjet23m        : 1.828e-02
                         :    9 : DRlepTm         : 1.647e-02
                         :   10 : DRjet31m        : 1.646e-02
                         :   11 : bjetmDR         : 1.637e-02
                         :   12 : dibjetsMass     : 1.628e-02
                         :   13 : jet3cvsb        : 1.624e-02
                         :   14 : DRjet3cvsb      : 1.615e-02
                         :   15 : DRjet2cvsl      : 1.598e-02
                         :   16 : jet1cvsb        : 1.563e-02
                         :   17 : DRjet0csv       : 1.556e-02
                         :   18 : DRjet12DR       : 1.529e-02
                         :   19 : jet3eta         : 1.513e-02
                         :   20 : DRjet3m         : 1.509e-02
                         :   21 : jet1eta         : 1.505e-02
                         :   22 : DRjet0eta       : 1.496e-02
                         :   23 : jet4cvsb        : 1.490e-02
                         :   24 : jet2eta         : 1.487e-02
                         :   25 : lepDPhi         : 1.476e-02
                         :   26 : DRjet3eta       : 1.405e-02
                         :   27 : jet2cvsb        : 1.384e-02
                         :   28 : DRhadTm         : 1.348e-02
                         :   29 : jet2csv         : 1.343e-02
                         :   30 : bjetmDEta       : 1.340e-02
                         :   31 : jet4eta         : 1.331e-02
                         :   32 : jet1cvsl        : 1.330e-02
                         :   33 : DRjet1cvsb      : 1.327e-02
                         :   34 : bjetmDPhi       : 1.312e-02
                         :   35 : jet3cvsl        : 1.300e-02
                         :   36 : DRjet2eta       : 1.272e-02
                         :   37 : jet2cvsl        : 1.268e-02
                         :   38 : jet4cvsl        : 1.266e-02
                         :   39 : DRjet1csv       : 1.265e-02
                         :   40 : DRlepTpt        : 1.251e-02
                         :   41 : njets           : 1.250e-02
                         :   42 : jet3csv         : 1.249e-02
                         :   43 : DRjet1eta       : 1.231e-02
                         :   44 : jet4csv         : 1.217e-02
                         :   45 : DRjet12pt       : 1.202e-02
                         :   46 : DRjet1m         : 1.201e-02
                         :   47 : DRjet1cvsl      : 1.201e-02
                         :   48 : DRjet23pt       : 1.187e-02
                         :   49 : DRlepWeta       : 1.183e-02
                         :   50 : DRlepWm         : 1.183e-02
                         :   51 : DRjet3pt        : 1.164e-02
                         :   52 : jet4pt          : 1.159e-02
                         :   53 : nbjets_m        : 1.149e-02
                         :   54 : DRjet0m         : 1.147e-02
                         :   55 : jet1csv         : 1.111e-02
                         :   56 : DRjet2m         : 1.105e-02
                         :   57 : DRjet31pt       : 1.081e-02
                         :   58 : DRlepTeta       : 1.062e-02
                         :   59 : cjetPt          : 1.050e-02
                         :   60 : DRjet0pt        : 1.034e-02
                         :   61 : DRjet1pt        : 1.024e-02
                         :   62 : jet1pt          : 9.954e-03
                         :   63 : missingET       : 9.640e-03
                         :   64 : bjetPt_dibjetsm : 9.567e-03
                         :   65 : DRjet12eta      : 9.383e-03
                         :   66 : jet1m           : 9.286e-03
                         :   67 : ncjets_m        : 9.030e-03
                         :   68 : DRjet23eta      : 8.960e-03
                         :   69 : DRhadTpt        : 8.735e-03
                         :   70 : DRhadTeta       : 8.299e-03
                         :   71 : jet4m           : 8.159e-03
                         :   72 : DRjet31eta      : 8.148e-03
                         :   73 : jet2pt          : 8.109e-03
                         :   74 : jet3m           : 7.936e-03
                         :   75 : jet2m           : 7.672e-03
                         :   76 : DRjet2pt        : 7.653e-03
                         :   77 : jet3pt          : 7.274e-03
                         : -------------------------------------------------
                         : No variable ranking supplied by classifier: PyKeras
Factory                  : === Destroy and recreate all methods via weight files for testing ===
                         : 
Factory                  : Test all methods
Factory                  : Test method: BDT for Classification performance
                         : 
BDT                      : [keras_Hct1] : Evaluation of BDT on testing sample (20000 events)
                         : Elapsed time for evaluation of 20000 events: 1.13 sec       
Factory                  : Test method: PyKeras for Classification performance
                         : 
                         : Load model from file: keras_Hct1/weights/TrainedModel_PyKeras.h5
Factory                  : Evaluate all methods
Factory                  : Evaluate classifier: BDT
                         : 
BDT                      : [keras_Hct1] : Loop over test events and fill histograms with classifier response...
                         : 
TFHandler_BDT            :        Variable               Mean               RMS       [        Min               Max ]
                         : ----------------------------------------------------------------------------------------------
                         :           njets:           4.9460           1.1103   [           4.0000           12.000 ]
                         :        nbjets_m:           3.0821          0.28662   [           3.0000           6.0000 ]
                         :        ncjets_m:          0.72042          0.80120   [           0.0000           6.0000 ]
                         :         lepDPhi:       -0.0046156           1.6069   [          -3.1414           3.1397 ]
                         :       missingET:           69.422           50.539   [          0.20591           652.79 ]
                         :         bjetmDR:           1.3356          0.52794   [          0.39663           3.3751 ]
                         :       bjetmDEta:         0.012835          0.91999   [          -3.1577           3.2662 ]
                         :       bjetmDPhi:         0.022474           1.1025   [          -3.1360           3.1305 ]
                         :     dibjetsMass:           98.987           51.578   [           17.607           1172.7 ]
                         : bjetPt_dibjetsm:           107.12           63.963   [           30.208           844.52 ]
                         :          cjetPt:           89.619           72.084   [           30.003           1240.1 ]
                         :          jet1pt:           143.93           87.344   [           36.619           1240.1 ]
                         :          jet2pt:           97.411           51.213   [           31.503           710.41 ]
                         :          jet3pt:           69.808           31.805   [           30.183           477.37 ]
                         :          jet4pt:           51.738           21.273   [           30.001           313.93 ]
                         :         jet1eta:        0.0078892           1.0571   [          -2.3998           2.3999 ]
                         :         jet2eta:        -0.010044           1.0777   [          -2.3985           2.4000 ]
                         :         jet3eta:         0.011939           1.0990   [          -2.3977           2.3993 ]
                         :         jet4eta:       -0.0025951           1.1315   [          -2.3999           2.3999 ]
                         :           jet1m:           246.38           216.28   [           41.573           2931.1 ]
                         :           jet2m:           166.95           127.60   [           32.825           1791.8 ]
                         :           jet3m:           122.38           88.138   [           31.027           1310.4 ]
                         :           jet4m:           93.560           65.997   [           30.419           1163.0 ]
                         :         jet1csv:          0.76277          0.32301   [         0.048907          0.99963 ]
                         :         jet2csv:          0.76393          0.32184   [         0.048527          0.99955 ]
                         :         jet3csv:          0.72399          0.33997   [         0.044938           1.0000 ]
                         :         jet4csv:          0.69062          0.34470   [         0.050798          0.99956 ]
                         :        jet1cvsl:          0.56458          0.48058   [         -0.70120          0.99613 ]
                         :        jet2cvsl:          0.58778          0.47584   [         -0.75300          0.99693 ]
                         :        jet3cvsl:          0.53810          0.50435   [         -0.65572          0.99719 ]
                         :        jet4cvsl:          0.47659          0.52084   [         -0.76474          0.99675 ]
                         :        jet1cvsb:         -0.28607          0.52369   [         -0.98344          0.84175 ]
                         :        jet2cvsb:         -0.29449          0.52856   [         -0.98373          0.84753 ]
                         :        jet3cvsb:         -0.23429          0.53867   [         -0.98280          0.85439 ]
                         :        jet4cvsb:         -0.17506          0.53615   [         -0.98567          0.84009 ]
                         :       DRlepWeta:        0.0043794          0.86966   [          -4.6270           4.6901 ]
                         :         DRlepWm:           94.264           48.959   [          0.62400           487.72 ]
                         :        DRjet0pt:           112.78           85.698   [           30.004           1240.1 ]
                         :       DRjet0eta:         0.010643           1.1687   [          -2.3999           2.4000 ]
                         :         DRjet0m:           15.145           9.9236   [           2.0735           129.02 ]
                         :       DRjet0csv:          0.67570          0.35571   [         0.048527          0.99950 ]
                         :      DRjet0cvsl:          0.44961          0.52322   [         -0.70120          0.99675 ]
                         :      DRjet0cvsb:         -0.17370          0.55159   [         -0.98419          0.84175 ]
                         :        DRjet1pt:           81.168           52.193   [           30.002           844.52 ]
                         :       DRjet1eta:        0.0046999          0.98993   [          -2.3979           2.3988 ]
                         :         DRjet1m:           11.832           6.8980   [           2.4876           171.05 ]
                         :       DRjet1csv:          0.97848         0.028104   [          0.85027           1.0000 ]
                         :      DRjet1cvsl:          0.87829          0.16570   [         -0.44040          0.99693 ]
                         :      DRjet1cvsb:         -0.71992          0.25272   [         -0.98590          0.78888 ]
                         :        DRjet2pt:           81.223           56.496   [           30.000           693.38 ]
                         :       DRjet2eta:        -0.013073           1.0515   [          -2.3994           2.3984 ]
                         :         DRjet2m:           11.975           7.7277   [           2.2844           112.28 ]
                         :       DRjet2csv:          0.93142         0.045266   [          0.84840          0.99943 ]
                         :      DRjet2cvsl:          0.81638          0.22172   [         -0.60097          0.99719 ]
                         :      DRjet2cvsb:         -0.42594          0.34295   [         -0.98302          0.75284 ]
                         :        DRjet3pt:           71.263           48.522   [           30.001           718.96 ]
                         :       DRjet3eta:        0.0021205           1.0873   [          -2.3995           2.3993 ]
                         :         DRjet3m:           10.915           6.6350   [          0.81517           120.73 ]
                         :       DRjet3csv:          0.44946          0.32020   [         0.047345          0.99868 ]
                         :      DRjet3cvsl:          0.14689          0.48797   [         -0.76474          0.99653 ]
                         :      DRjet3cvsb:          0.20998          0.37862   [         -0.97331          0.85439 ]
                         :       DRjet12pt:           136.71           77.972   [           3.1064           857.57 ]
                         :      DRjet12eta:       -0.0057470           1.1426   [          -4.8784           4.9330 ]
                         :        DRjet12m:           107.62           56.994   [           17.607           1172.7 ]
                         :       DRjet12DR:           1.4964          0.61262   [          0.39915           4.0245 ]
                         :       DRjet23pt:           122.68           75.734   [          0.78451           890.16 ]
                         :      DRjet23eta:       -0.0084478           1.2751   [          -5.0996           5.7646 ]
                         :        DRjet23m:           106.36           67.744   [           16.851           1345.9 ]
                         :       DRjet31pt:           121.51           73.878   [          0.64069           831.75 ]
                         :      DRjet31eta:        0.0039146           1.2396   [          -5.2307           6.7048 ]
                         :        DRjet31m:           108.11           64.567   [           17.970           966.44 ]
                         :        DRlepTpt:           162.53           97.428   [           1.0634           1010.3 ]
                         :       DRlepTeta:        0.0076308           1.2158   [          -4.9414           5.9664 ]
                         :         DRlepTm:           252.57           156.71   [           27.131           2012.9 ]
                         :        DRhadTpt:           174.20           94.309   [           3.8167           937.67 ]
                         :       DRhadTeta:       -0.0040500           1.2421   [          -4.4400           4.5573 ]
                         :         DRhadTm:           194.16           91.317   [           33.172           1570.7 ]
                         : ----------------------------------------------------------------------------------------------
                         : 
                         : <PlotVariables> Will not produce scatter plots ==> 
                         : |  The number of 77 input variables and 0 target values would require 2926 two-dimensional
                         : |  histograms, which would occupy the computer's memory. Note that this
                         : |  suppression does not have any consequences for your analysis, other
                         : |  than not disposing of these scatter plots. You can modify the maximum
                         : |  number of input variables allowed to generate scatter plots in your
                         : |  script via the command line:
                         : |  "(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;"
                         : 
                         : Some more output
Factory                  : Evaluate classifier: PyKeras
                         : 
TFHandler_PyKeras        :        Variable               Mean               RMS       [        Min               Max ]
                         : ----------------------------------------------------------------------------------------------
                         :           njets:       0.00073032          0.98902   [          -3.1189           3.5486 ]
                         :        nbjets_m:        -0.019800           1.0110   [          -3.3206           3.8158 ]
                         :        ncjets_m:        0.0058321           1.0042   [          -3.1528           5.7307 ]
                         :         lepDPhi:        0.0016265          0.99492   [          -3.4281           3.4997 ]
                         :       missingET:         0.018866          0.99759   [          -3.1969           5.7307 ]
                         :         bjetmDR:        -0.014041          0.99282   [          -3.1443           5.7307 ]
                         :       bjetmDEta:        0.0053678          0.98287   [          -3.3603           4.3843 ]
                         :       bjetmDPhi:         0.022564          0.97367   [          -3.2094           5.7307 ]
                         :     dibjetsMass:        -0.026506          0.97803   [          -3.2300           5.7307 ]
                         : bjetPt_dibjetsm:         0.021407          0.99086   [          -3.1569           5.7307 ]
                         :          cjetPt:       -0.0071419          0.99392   [          -3.0218           5.7307 ]
                         :          jet1pt:       -0.0031759          0.98609   [          -3.1558           3.6166 ]
                         :          jet2pt:       0.00065979          0.98349   [          -3.1551           3.4354 ]
                         :          jet3pt:       -0.0030729          0.98033   [          -3.1660           4.2290 ]
                         :          jet4pt:       0.00042609          0.98692   [          -3.0513           5.7307 ]
                         :         jet1eta:        0.0057729          0.98457   [          -3.1247           3.4218 ]
                         :         jet2eta:       -0.0019578          0.99768   [          -3.1231           4.3538 ]
                         :         jet3eta:         0.013661          0.97548   [          -5.7307           3.6349 ]
                         :         jet4eta:        0.0015135          0.97928   [          -3.1400           3.5893 ]
                         :           jet1m:        -0.014105          0.99071   [          -3.1618           3.6359 ]
                         :           jet2m:         0.018473          0.97823   [          -3.1263           3.6552 ]
                         :           jet3m:       0.00017779          0.98885   [          -3.1694           5.7307 ]
                         :           jet4m:       -0.0045471          0.98224   [          -3.0139           5.7307 ]
                         :         jet1csv:        0.0093997          0.96920   [          -3.0144           3.3541 ]
                         :         jet2csv:        0.0052741          0.97723   [          -3.0521           5.7307 ]
                         :         jet3csv:        0.0067857          0.98732   [          -3.1848           3.9828 ]
                         :         jet4csv:         0.029241          0.99097   [          -3.1889           5.7307 ]
                         :        jet1cvsl:         0.016637          0.97696   [          -3.2245           3.4982 ]
                         :        jet2cvsl:         0.025331          0.98628   [          -3.1036           5.7307 ]
                         :        jet3cvsl:         0.025053          0.99944   [          -3.0933           5.7307 ]
                         :        jet4cvsl:        0.0050250          0.99848   [          -5.7307           5.7307 ]
                         :        jet1cvsb:        -0.012605          0.98820   [          -3.2414           5.7307 ]
                         :        jet2cvsb:        -0.021989           1.0038   [          -3.0737           4.1089 ]
                         :        jet3cvsb:        -0.010429           1.0005   [          -3.8366           5.7307 ]
                         :        jet4cvsb:        -0.017713           1.0066   [          -5.7307           4.0915 ]
                         :       DRlepWeta:       0.00053433          0.98543   [          -3.1567           3.9285 ]
                         :         DRlepWm:       -0.0022293           1.0023   [          -3.2019           3.7639 ]
                         :        DRjet0pt:        0.0081622          0.99152   [          -3.0559           3.6755 ]
                         :       DRjet0eta:         0.012583          0.99447   [          -3.2203           5.7307 ]
                         :         DRjet0m:        0.0071205          0.99579   [          -3.1593           3.2201 ]
                         :       DRjet0csv:        0.0090852          0.98540   [          -3.1298           5.7307 ]
                         :      DRjet0cvsl:       -0.0037476          0.99151   [          -3.6054           3.8158 ]
                         :      DRjet0cvsb:        -0.010725           1.0030   [          -3.1662           4.3600 ]
                         :        DRjet1pt:         0.011840          0.97780   [          -3.3832           5.7307 ]
                         :       DRjet1eta:        0.0091225          0.98382   [          -3.1412           3.6931 ]
                         :         DRjet1m:        -0.010176          0.99598   [          -3.2843           5.7307 ]
                         :       DRjet1csv:       -0.0038457          0.98050   [          -5.7307           3.8206 ]
                         :      DRjet1cvsl:        0.0019591          0.98295   [          -3.0842           5.7307 ]
                         :      DRjet1cvsb:        -0.017139          0.98533   [          -3.1626           3.4882 ]
                         :        DRjet2pt:         0.013120          0.98304   [          -3.2240           3.2340 ]
                         :       DRjet2eta:       -0.0098424          0.98472   [          -3.3265           3.5514 ]
                         :         DRjet2m:        0.0042426          0.98855   [          -3.0267           3.2006 ]
                         :       DRjet2csv:         0.027113          0.99069   [          -3.0674           3.4280 ]
                         :      DRjet2cvsl:         0.018960          0.97406   [          -3.0939           3.3724 ]
                         :      DRjet2cvsb:        -0.040335          0.99203   [          -3.2773           5.7307 ]
                         :        DRjet3pt:       -0.0073301          0.97674   [          -3.1695           3.4968 ]
                         :       DRjet3eta:         0.011125          0.99525   [          -3.0733           5.7307 ]
                         :         DRjet3m:         0.025415          0.98405   [          -3.0982           3.3816 ]
                         :       DRjet3csv:         0.010991           1.0004   [          -3.0989           4.2641 ]
                         :      DRjet3cvsl:         0.049153           1.0102   [          -3.0999           3.6194 ]
                         :      DRjet3cvsb:       -0.0024263           1.0034   [          -3.3552           3.3671 ]
                         :       DRjet12pt:        0.0064656          0.98631   [          -3.2622           5.7307 ]
                         :      DRjet12eta:       -0.0014161          0.98208   [          -3.1692           3.3616 ]
                         :        DRjet12m:       -0.0016439          0.97949   [          -3.5038           5.7307 ]
                         :       DRjet12DR:         0.011176           1.0017   [          -3.2672           3.4629 ]
                         :       DRjet23pt:        0.0031648          0.97593   [          -3.0508           3.4945 ]
                         :      DRjet23eta:       -0.0023924          0.98504   [          -3.1749           3.6414 ]
                         :        DRjet23m:        0.0048186          0.98005   [          -3.0987           5.7307 ]
                         :       DRjet31pt:        0.0041266          0.97749   [          -3.4364           5.7307 ]
                         :      DRjet31eta:        0.0023416          0.98327   [          -3.0313           5.7307 ]
                         :        DRjet31m:        -0.015248          0.97055   [          -3.3038           3.5162 ]
                         :        DRlepTpt:        0.0081138          0.99098   [          -3.0704           5.7307 ]
                         :       DRlepTeta:       -0.0011483          0.98358   [          -3.1259           5.7307 ]
                         :         DRlepTm:       -0.0062858          0.99692   [          -5.7307           5.7307 ]
                         :        DRhadTpt:        0.0034227          0.97958   [          -5.7307           3.8022 ]
                         :       DRhadTeta:       -0.0014542          0.97434   [          -3.1921           3.7629 ]
                         :         DRhadTm:        -0.028946          0.98366   [          -3.0553           5.7307 ]
                         : ----------------------------------------------------------------------------------------------
PyKeras                  : [keras_Hct1] : Loop over test events and fill histograms with classifier response...
                         : 
TFHandler_PyKeras        :        Variable               Mean               RMS       [        Min               Max ]
                         : ----------------------------------------------------------------------------------------------
                         :           njets:       0.00073032          0.98902   [          -3.1189           3.5486 ]
                         :        nbjets_m:        -0.019800           1.0110   [          -3.3206           3.8158 ]
                         :        ncjets_m:        0.0058321           1.0042   [          -3.1528           5.7307 ]
                         :         lepDPhi:        0.0016265          0.99492   [          -3.4281           3.4997 ]
                         :       missingET:         0.018866          0.99759   [          -3.1969           5.7307 ]
                         :         bjetmDR:        -0.014041          0.99282   [          -3.1443           5.7307 ]
                         :       bjetmDEta:        0.0053678          0.98287   [          -3.3603           4.3843 ]
                         :       bjetmDPhi:         0.022564          0.97367   [          -3.2094           5.7307 ]
                         :     dibjetsMass:        -0.026506          0.97803   [          -3.2300           5.7307 ]
                         : bjetPt_dibjetsm:         0.021407          0.99086   [          -3.1569           5.7307 ]
                         :          cjetPt:       -0.0071419          0.99392   [          -3.0218           5.7307 ]
                         :          jet1pt:       -0.0031759          0.98609   [          -3.1558           3.6166 ]
                         :          jet2pt:       0.00065979          0.98349   [          -3.1551           3.4354 ]
                         :          jet3pt:       -0.0030729          0.98033   [          -3.1660           4.2290 ]
                         :          jet4pt:       0.00042609          0.98692   [          -3.0513           5.7307 ]
                         :         jet1eta:        0.0057729          0.98457   [          -3.1247           3.4218 ]
                         :         jet2eta:       -0.0019578          0.99768   [          -3.1231           4.3538 ]
                         :         jet3eta:         0.013661          0.97548   [          -5.7307           3.6349 ]
                         :         jet4eta:        0.0015135          0.97928   [          -3.1400           3.5893 ]
                         :           jet1m:        -0.014105          0.99071   [          -3.1618           3.6359 ]
                         :           jet2m:         0.018473          0.97823   [          -3.1263           3.6552 ]
                         :           jet3m:       0.00017779          0.98885   [          -3.1694           5.7307 ]
                         :           jet4m:       -0.0045471          0.98224   [          -3.0139           5.7307 ]
                         :         jet1csv:        0.0093997          0.96920   [          -3.0144           3.3541 ]
                         :         jet2csv:        0.0052741          0.97723   [          -3.0521           5.7307 ]
                         :         jet3csv:        0.0067857          0.98732   [          -3.1848           3.9828 ]
                         :         jet4csv:         0.029241          0.99097   [          -3.1889           5.7307 ]
                         :        jet1cvsl:         0.016637          0.97696   [          -3.2245           3.4982 ]
                         :        jet2cvsl:         0.025331          0.98628   [          -3.1036           5.7307 ]
                         :        jet3cvsl:         0.025053          0.99944   [          -3.0933           5.7307 ]
                         :        jet4cvsl:        0.0050250          0.99848   [          -5.7307           5.7307 ]
                         :        jet1cvsb:        -0.012605          0.98820   [          -3.2414           5.7307 ]
                         :        jet2cvsb:        -0.021989           1.0038   [          -3.0737           4.1089 ]
                         :        jet3cvsb:        -0.010429           1.0005   [          -3.8366           5.7307 ]
                         :        jet4cvsb:        -0.017713           1.0066   [          -5.7307           4.0915 ]
                         :       DRlepWeta:       0.00053433          0.98543   [          -3.1567           3.9285 ]
                         :         DRlepWm:       -0.0022293           1.0023   [          -3.2019           3.7639 ]
                         :        DRjet0pt:        0.0081622          0.99152   [          -3.0559           3.6755 ]
                         :       DRjet0eta:         0.012583          0.99447   [          -3.2203           5.7307 ]
                         :         DRjet0m:        0.0071205          0.99579   [          -3.1593           3.2201 ]
                         :       DRjet0csv:        0.0090852          0.98540   [          -3.1298           5.7307 ]
                         :      DRjet0cvsl:       -0.0037476          0.99151   [          -3.6054           3.8158 ]
                         :      DRjet0cvsb:        -0.010725           1.0030   [          -3.1662           4.3600 ]
                         :        DRjet1pt:         0.011840          0.97780   [          -3.3832           5.7307 ]
                         :       DRjet1eta:        0.0091225          0.98382   [          -3.1412           3.6931 ]
                         :         DRjet1m:        -0.010176          0.99598   [          -3.2843           5.7307 ]
                         :       DRjet1csv:       -0.0038457          0.98050   [          -5.7307           3.8206 ]
                         :      DRjet1cvsl:        0.0019591          0.98295   [          -3.0842           5.7307 ]
                         :      DRjet1cvsb:        -0.017139          0.98533   [          -3.1626           3.4882 ]
                         :        DRjet2pt:         0.013120          0.98304   [          -3.2240           3.2340 ]
                         :       DRjet2eta:       -0.0098424          0.98472   [          -3.3265           3.5514 ]
                         :         DRjet2m:        0.0042426          0.98855   [          -3.0267           3.2006 ]
                         :       DRjet2csv:         0.027113          0.99069   [          -3.0674           3.4280 ]
                         :      DRjet2cvsl:         0.018960          0.97406   [          -3.0939           3.3724 ]
                         :      DRjet2cvsb:        -0.040335          0.99203   [          -3.2773           5.7307 ]
                         :        DRjet3pt:       -0.0073301          0.97674   [          -3.1695           3.4968 ]
                         :       DRjet3eta:         0.011125          0.99525   [          -3.0733           5.7307 ]
                         :         DRjet3m:         0.025415          0.98405   [          -3.0982           3.3816 ]
                         :       DRjet3csv:         0.010991           1.0004   [          -3.0989           4.2641 ]
                         :      DRjet3cvsl:         0.049153           1.0102   [          -3.0999           3.6194 ]
                         :      DRjet3cvsb:       -0.0024263           1.0034   [          -3.3552           3.3671 ]
                         :       DRjet12pt:        0.0064656          0.98631   [          -3.2622           5.7307 ]
                         :      DRjet12eta:       -0.0014161          0.98208   [          -3.1692           3.3616 ]
                         :        DRjet12m:       -0.0016439          0.97949   [          -3.5038           5.7307 ]
                         :       DRjet12DR:         0.011176           1.0017   [          -3.2672           3.4629 ]
                         :       DRjet23pt:        0.0031648          0.97593   [          -3.0508           3.4945 ]
                         :      DRjet23eta:       -0.0023924          0.98504   [          -3.1749           3.6414 ]
                         :        DRjet23m:        0.0048186          0.98005   [          -3.0987           5.7307 ]
                         :       DRjet31pt:        0.0041266          0.97749   [          -3.4364           5.7307 ]
                         :      DRjet31eta:        0.0023416          0.98327   [          -3.0313           5.7307 ]
                         :        DRjet31m:        -0.015248          0.97055   [          -3.3038           3.5162 ]
                         :        DRlepTpt:        0.0081138          0.99098   [          -3.0704           5.7307 ]
                         :       DRlepTeta:       -0.0011483          0.98358   [          -3.1259           5.7307 ]
                         :         DRlepTm:       -0.0062858          0.99692   [          -5.7307           5.7307 ]
                         :        DRhadTpt:        0.0034227          0.97958   [          -5.7307           3.8022 ]
                         :       DRhadTeta:       -0.0014542          0.97434   [          -3.1921           3.7629 ]
                         :         DRhadTm:        -0.028946          0.98366   [          -3.0553           5.7307 ]
                         : ----------------------------------------------------------------------------------------------
                         : 
                         : <PlotVariables> Will not produce scatter plots ==> 
                         : |  The number of 77 input variables and 0 target values would require 2926 two-dimensional
                         : |  histograms, which would occupy the computer's memory. Note that this
                         : |  suppression does not have any consequences for your analysis, other
                         : |  than not disposing of these scatter plots. You can modify the maximum
                         : |  number of input variables allowed to generate scatter plots in your
                         : |  script via the command line:
                         : |  "(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;"
                         : 
                         : Some more output
                         : 
                         : Evaluation results ranked by best signal efficiency and purity (area)
                         : -------------------------------------------------------------------------------------------------------------------
                         : DataSet       MVA                       
                         : Name:         Method:          ROC-integ
                         : keras_Hct1    BDT            : 0.748
                         : keras_Hct1    PyKeras        : 0.740
                         : -------------------------------------------------------------------------------------------------------------------
                         : 
                         : Testing efficiency compared to training efficiency (overtraining check)
                         : -------------------------------------------------------------------------------------------------------------------
                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) 
                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   
                         : -------------------------------------------------------------------------------------------------------------------
                         : keras_Hct1           BDT            : 0.122 (0.124)       0.396 (0.411)      0.657 (0.674)
                         : keras_Hct1           PyKeras        : 0.106 (0.118)       0.378 (0.402)      0.647 (0.661)
                         : -------------------------------------------------------------------------------------------------------------------
                         : 
Dataset:keras_Hct1       : Created tree 'TestTree' with 20000 events
                         : 
Dataset:keras_Hct1       : Created tree 'TrainTree' with 70000 events
                         : 
Factory                  : Thank you for using TMVA!
